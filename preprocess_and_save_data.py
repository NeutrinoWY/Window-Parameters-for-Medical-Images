# -*- coding: utf-8 -*-
"""preprocess_and_save_data.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wbqi9i6wIgmGMJcz3DPXafS0BTbnLPLJ
"""

import numpy as np
import pandas as pd
from keras.utils import to_categorical
import nibabel as nib
import h5py
import os


"""**Read and Save flair images data into h5 file ( 235 patients who have survival days info)**"""

def save_images_data(dirpath="/content/drive/MyDrive/Master's Project/flair_235",filename="images_data.h5"):
  # get all the filepaths
  file_paths = []
  for dirp, sf, files in os.walk(dirpath):
    for file in sorted(files):
      file_paths.append(os.path.join(dirp, file))
  print(file_paths)

  # create a h5 file
  h5f = h5py.File(filename, "w")
  dset = h5f.create_dataset('X', (len(file_paths), 240, 240, 155))

  # read image data and write it into the h5 file incrementally
  for i, f in enumerate(file_paths):
    data = nib.load(f)
    image_data = data.get_fdata()
    #print("saving", x, flush=True)
    dset[i] = image_data

  h5f.close()

save_images_data()

!cp images_data.h5 "/content/drive/MyDrive/Master's Project/data"

"""**read and save seg images which have corresponding survival days (235 patients)**"""

save_images_data(dirpath="/content/drive/MyDrive/Master's Project/seg_235",filename="seg_data_235.h5")

!cp seg_data_235.h5 "/content/drive/MyDrive/Master's Project/data"

"""load data and check element values of seg image"""

data_path = "/content/drive/MyDrive/Master's Project/data/seg_data_235.h5"
h5f = h5py.File(data_path,'r')

set(h5f['X'][0:50].flatten())

set(h5f['X'][50:100].flatten())

set(h5f['X'][200:235].flatten())

"""**calculate and save tumor proportion of each slices of the 235 patients**"""

# calculate the tumor proportion of each slice of one patient
def calculate_tumor_proportion(seg_data):
  """
  calculate the tumor proportion on each slice of each patient.
  input args: seg_data: the segmentation images data
  output: numpy array of tumor proportion of each slice of each patient. 
  """
  tumor_proportion = [0]*155
  for i in range(0,155):                 # ith slice
      if i==0:
        print(seg_data.shape)
      tp = np.sum(seg_data[:,:,i]>0, axis=(0,1)) / (240*240)        
      tumor_proportion[i] = tp 
  
  return tumor_proportion

def save_tumor_proprotion_data(dirpath="/content/drive/MyDrive/Master's Project/seg_235",filename="tumor_proportion.h5"):
  """
  read seg image one by one, 
  calculate the tumor proportion of the 155 slices of each seg image,
  save the tumor proportion data in a h5 file incrementally.
  The shape of dataset in h5 file is 235*155. 
  """
  # get all the filepaths of the seg images
  file_paths = []
  for dirp, sf, files in os.walk(dirpath):
    for file in sorted(files):
      file_paths.append(os.path.join(dirp, file))
  print(file_paths)

  # create a h5 file
  h5f = h5py.File(filename, "w")
  dset = h5f.create_dataset('tp', (len(file_paths), 155))

  # read image data
  for i, f in enumerate(file_paths):
    data = nib.load(f)
    seg_data = data.get_fdata()
    # calculate the tumor proportion of all slices of one seg image
    tp = calculate_tumor_proportion(seg_data)
    #print("saving", x, flush=True)
    # save the 155 proportion numbers of this seg image in dset
    dset[i] = tp

  h5f.close()

save_tumor_proprotion_data()

# copy the file into my drive
!cp tumor_proportion.h5 "/content/drive/MyDrive/Master's Project/data"

"""**Get flair slices with tumor bigger than threshold, save the tumor slices in h5 file**"""

# get tumor proportion data
tumor_prop_path = "/content/drive/MyDrive/Master's Project/data/tumor_proportion.h5"
h5f = h5py.File(tumor_prop_path,'r')
tumor_prop = h5f['tp'][:]
tumor_prop.shape     # (235,155)

# check
tumor_prop[0]

# number of tumor slices of the first patient
len(np.where(tumor_prop[0]>0)[0])

def extract_tumor_slices_data(tumor_prop, threshold=0, dirpath="/content/drive/MyDrive/Master's Project/flair_235"):
  # get all the filepaths
  file_paths = []
  for dirp, sf, files in os.walk(dirpath):
    for file in sorted(files):
      file_paths.append(os.path.join(dirp, file))
  print(file_paths)

  # calculate how many slices in total with tumor proportion bigger than threshold
  n = len(np.where(tumor_prop.flatten() > threshold)[0])
  print(n) 

  # create h5 file
  h5f = h5py.File('tumor_slices_data.h5','a')
  dset = h5f.create_dataset('tumor_slices', (n,240,240,1))  

  # read image data and write it into the h5 file incrementally
  start=0
  for i, f in enumerate(file_paths):
    #  store the tumor slices data of one patient in each iteration 
    tumor_slices_data = []

    # read flair data of one patient in each iteration
    data = nib.load(f)
    image_data = data.get_fdata()

    # get the indice of tumor slices of this one patient
    tumor_indice = np.where(tumor_prop[i]>threshold)[0]
    # get the number of tumor slices of this one patient
    count_tm = len(tumor_indice)
    end = start + count_tm

    # extract the flair tumor slices
    for idx in tumor_indice:
      tumor_slices_data.append(image_data[:,:,idx].reshape(240,240,1))
    
    # save the tumor slices data of this patient into h5 file
    dset[start:end] = tumor_slices_data
    start = end

  return

extract_tumor_slices_data(tumor_prop)

# copy the file into my drive
!cp tumor_slices_data.h5 "/content/drive/MyDrive/Master's Project/data"

"""**prepare corresponding labels for each tumor slices, save them into h5 file**"""

# read label data
df = pd.read_csv("/content/drive/MyDrive/Master's Project/survival_info.csv")

# remove #84 paient's data, as this patient has no exact 'survival days'
df = df.drop(83,axis=0)

def categorize_survival_days(days):
  """days: positive integer."""
  assert days>0, "days should be a postive number"
  if days <= 240:
    categ = 0
  elif days >= 450:
    categ = 2
  else: 
    categ = 1
  return categ

df['Survival_category'] = [categorize_survival_days(int(days)) for days in df['Survival_days']]

survival = np.array(df['Survival_category'])
survival

# count the number of tumor slices of each patient
def count_tumor_slices(tumor_prop,threshold=0):
  count_tm_sli = [0]*tumor_prop.shape[0]
  for i, props in enumerate(tumor_prop):
    count = len(np.where(props>threshold)[0])
    count_tm_sli[i] = count
  return count_tm_sli

count = count_tumor_slices(tumor_prop)
count

# expand the survival label for the tumor slices images
def expand_labels(original_label, count):
  """
  As we separate the 3d images to 2d tumor slices,
  we also need to tag each tumor slices with the corresponding label.
  args: 
  original_label:  number encoded label of the 235 patients.
  count: list, number of tumor slices of each patient

  output: the expanded one hot encoded labels for the tumor slices.
  """
  survival_labels = []
  for i, number in enumerate(count):
    survival_labels.append([survival[i]]*number)
  print(survival_labels[0:2])

  labels = [item for sublist in survival_labels for item in sublist]
  labels = pd.DataFrame(labels)
  print(labels)

  # one hot encode the labels
  y = to_categorical(labels)

  return y

y = expand_labels(survival, count)

y.shape

y[0]

"""**write tumor slices data and their corresponding one hot encoded labels into one h5 file**"""

# add labels into the tumor_slices_data file
with h5py.File('tumor_slices_data.h5','a') as h5f:
    h5f.create_dataset('labels', data=y)

print ('done')

sli.dtype

"""**get max intensity value of each patient's MRI**"""

def getMaxValue(dirpath="/content/drive/MyDrive/Master's Project/flair_235"):
  # get all the filepaths
  file_paths = []
  for dirp, sf, files in os.walk(dirpath):
    for file in sorted(files):
      file_paths.append(os.path.join(dirp, file))
  print(file_paths)

  max_values = [0]*235
  for i, f in enumerate(file_paths):
    data = nib.load(f)
    image_data = data.get_fdata()
    max_values[i] = int(np.max(image_data))
    if i == 10:
      print(max_values[i])
  

  return max_values

  
max_values = getMaxValue()

max_values

max(max_values)

"""**max values for the tumor slices of each patient**"""

expand_max_values = []
for i in range(235):
    expand_max_values.append([max_values[i]]*counts[i])
print(expand_max_values[0])

expand_max_values = [item for sublist in expand_max_values for item in sublist]

len(expand_max_values)

with h5py.File('tumor_slices_data.h5','a') as h5f:
    h5f.create_dataset('max_HU', data=expand_max_values)

#h5f.close()

print ('done')

# check the final file
h5f = h5py.File('tumor_slices_data.h5','r')
sli = h5f['tumor_slices'][82:84]
y = h5f['labels'][82:84]
mx_hu = h5f['max_HU'][82:84]
print(sli.shape)
print(y)
print(mx_hu)

# copy the file into my drive
!cp tumor_slices_data.h5 "/content/drive/MyDrive/Master's Project/data"