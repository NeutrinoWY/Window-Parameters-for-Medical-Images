# -*- coding: utf-8 -*-
"""pre_trained_model_v3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10Xeer61pTE1N0ZRk9iwWHty6A5Q9i-GX

input: slice with tumor (240,240,1) ->
WSO: one filter, one window setting -> windowed image (240,240,1)
->repeat windowed image to form 3 channels (240,240,3) ->
pre-trained image cnn to do clf.
"""

import pandas as pd
import numpy as np
import nibabel as nib
import copy
import h5py
import os
import matplotlib.pyplot as plt
import math

import warnings
warnings.filterwarnings("ignore",category=DeprecationWarning)

import sklearn
import tensorflow as tf
keras = tf.keras
K = keras.backend
#from keras.optimizers import SGD, Adam
#from keras.callbacks import ModelCheckpoint, LearningRateScheduler, TensorBoard, Callback
#from keras.models import Model, model_from_json
#from keras.applications.inception_v3 import InceptionV3
from sklearn.model_selection import train_test_split

"""**Hyperparameter**"""

BATCH_SIZE = 32
EARLY_STOP = 10
EPOCHS = 200
RANDOM_STATE = 42

UPBOUND_VALUE = 255.0
WINDOW_FUNCTION = "relu"   # or 'sigmoid'
RESCALE = "none"   # "patient max", "none", "global max", "by slice"

# Initiative window setting with default window parameters for brain image
WW = 450
WL = 225

"""# **DEFINE FUNCTIONS**

**Funtions for load and rescale data**
"""

def load_data(data_path="/content/drive/MyDrive/Master's Project/data/tumor_slices_data.h5", start=0,end=11000):
  """
  load data from h5 file, then shuffle data.
  """
  # load data from h5 file
  h5f = h5py.File(data_path,'r')

  # get data
  tm_slices = np.array(h5f['tumor_slices'])[start:end]
  Y = np.array(h5f['labels'])[start:end]
  max_hus = np.array(h5f['max_HU'][start:end])

  # shuffle data
  tm_slices, Y, max_hus = sklearn.utils.shuffle(tm_slices, Y, max_hus, random_state=RANDOM_STATE)

  
  return tm_slices, Y, max_hus


def max_min_pixel(X):
  max_val = 0
  min_val = 0
  for i, img in enumerate(X):
    max_val = max(max_val,  np.max(img))
    min_val = min(min_val, np.min(img))
    if np.min(img) != 0:
      print("min val of the slice is not zero")
  return max_val, min_val
  # max_val = 2291, min_val = 0


def rescale_data(X):
  """
      normalize data by global min/max value. 
  """
  X = X.astype(np.float32)
  X_rescale = [0]*len(X)
  for i, img in enumerate(X):
    min_val = 0
    max_val = 2291
    img = 255.0*(img-min_val)/(max_val-min_val+1e-10)
    img = img.astype(np.float32)
    X_rescale[i] = img

  return np.array(X_rescale)


def rescale_data_for_pat(X, max_hus):
  """
  rescale slice by min/max value of the patient MRI.
  """
  X = X.astype(np.float32)
  X_rescale = [0]*len(X)
  for i, img in enumerate(X):
    min_val = 0
    max_val = max_hus[i]
    img = 255.0*(img-min_val)/(max_val-min_val+1e-10)
    img = img.astype(np.float32)
    X_rescale[i] = img

  return np.array(X_rescale)



def rescale_data_by_slice(X):
  """
  rescale by slice's min/max value.
  """
  X = X.astype(np.float32)
  X_rescale = [0]*len(X)
  for i, img in enumerate(X):
    min_val = 0
    max_val = np.max(img)
    img = 255.0*(img-min_val)/(max_val-min_val+1e-10)
    img = img.astype(np.float32)
    X_rescale[i] = img

  return np.array(X_rescale)




class DataSequence(tf.keras.utils.Sequence):

      def __init__(self, X, Y):
          self.X = X
          self.Y = Y

      def __len__(self):
          return math.ceil(len(self.X) / BATCH_SIZE)

      def __getitem__(self, idx):
          return [self.X[idx*BATCH_SIZE:(idx + 1)*BATCH_SIZE], self.Y[idx*BATCH_SIZE:(idx + 1)*BATCH_SIZE]]

"""**Define WSO**"""

# define convolutional layer for wso
def WinOptConv(nch_window=1, **kwargs):
    conv_layer = keras.layers.Conv2D(filters=nch_window, kernel_size=(1, 1), strides=(1, 1), padding="valid",
                        name="window_conv_layer",**kwargs)
    return conv_layer

# define activation for wso
def WinOptActivation(upbound_window=UPBOUND_VALUE, window_function=WINDOW_FUNCTION):

    def upbound_sigmoid(x):
      return upbound_window * K.sigmoid(x)

    def upbound_relu(x):
      return K.minimum(K.maximum(x, 0), upbound_window)

    if window_function == "relu":
      act_layer = keras.layers.Activation(upbound_relu, name="window_act_layer")
    if window_function == "sigmoid":
      act_layer = keras.layers.Activation(upbound_sigmoid, name="window_act_layer")
    
    return act_layer


# wso with one convolutional layer and activation function
def WindowOptimizer(upbound_window=UPBOUND_VALUE, window_function=WINDOW_FUNCTION, nch_window=1, **kwargs):

    conv_layer = WinOptConv(nch_window=nch_window, **kwargs)
    act_layer = WinOptActivation(upbound_window=upbound_window, window_function= window_function)

    ## Return layer funcion
    def window_func(x):
        x = conv_layer(x)
        x = act_layer(x)
        return x

    return window_func

"""**Convert window parameters to weights of WSO, initialize WSO weights**"""

# change window parameters to conv params
def get_init_conv_params_relu(wl, ww, upbound_value=UPBOUND_VALUE):
    w = upbound_value / ww
    b = -1. * upbound_value * (wl - ww / 2.) / ww
    return (w, b)

def get_init_conv_params_sigmoid(wl, ww, smooth=None, upbound_value=UPBOUND_VALUE):
    if smooth is None:
      smooth = upbound_value / 255.0

    w = 2./ww * np.log(upbound_value/smooth - 1.)
    b = -2.* wl / ww * np.log(upbound_value/smooth - 1.)
    return (w, b)


# initialize conv parameters with the WW and WL  
def initialize_window_setting(model, wl=WL, ww=WW, upbound_window=UPBOUND_VALUE, window_function=WINDOW_FUNCTION):

    layer_names = [layer.name for layer in model.layers]
    if window_function == "relu":
      w_new, b_new = get_init_conv_params_relu(wl, ww, upbound_window)
    if window_function == "sigmoid":
      w_new, b_new = get_init_conv_params_sigmoid(wl, ww,upbound_window)

    w_conv_ori, b_conv_ori = model.layers[layer_names.index("window_conv_layer")].get_weights()
    w_conv_new = np.zeros_like(w_conv_ori)
    w_conv_new[0,0,0,:] = w_new * np.ones(w_conv_ori.shape[-1], dtype=w_conv_ori.dtype)
    b_conv_new = b_new * np.ones(b_conv_ori.shape, dtype=b_conv_ori.dtype)

    model.layers[layer_names.index("window_conv_layer")].set_weights([w_conv_new, b_conv_new])

    return model

"""**Compile Model (WSO + pre-trained CNN)**"""

# compile model 
input_shape = (240,240,1)
input_tensor = keras.layers.Input(shape=input_shape, name="input")

# wso, output of this layer is windowed image
x = WindowOptimizer(nch_window=1, upbound_window=UPBOUND_VALUE,
                    window_function=WINDOW_FUNCTION,
                    kernel_initializer="he_normal",
                    kernel_regularizer=keras.regularizers.l2(0.5 * 1e-5)
                    )(input_tensor)

# repeat windowed image data to form 3 channels
x = keras.layers.concatenate([x,x,x], axis=-1, name="repeat_to_3_channels_layer")


# use pre-trained model
x = keras.applications.InceptionV3(input_shape=(240,240,3),
                                          include_top=False,
                                          weights='imagenet')(x)
x = keras.layers.Flatten()(x)
x = keras.layers.Dense(128,activation='relu')(x)
outputs = keras.layers.Dense(3, activation='softmax')(x)

model = keras.models.Model(inputs=input_tensor, outputs=outputs, name="main_model")
    
## Initialize parameters of window setting opt module
model = initialize_window_setting(model)

optimizer = keras.optimizers.SGD(lr=0.001, decay=0, momentum=0.9, nesterov=True)
model.compile(optimizer=optimizer, loss='CategoricalCrossentropy', metrics=["accuracy"])
model.summary()

"""**Get weights and convert weights to window parameters**"""

## get the weights of the WSO convolutional layer
def get_weights(model, layer_name="window_conv_layer"):
  names = [weight.name for layer in model.layers for weight in layer.weights]
  #print(names)
  weights = model.get_weights()

  for name, weight in zip(names, weights):
    if "window_conv_layer" in name:
      if "kernel:0" in name:
        ws = weight
      if "bias:0" in name:
        bs = weight

  #print("w={} b={}".format(ws[0, 0, 0, :], bs))
  #print(ws)
  #print(ws.shape)
  w = ws[0,0,0,:]
  b = bs
  print("w={},b={}".format(w,b))
  return w, b

# convert weights w,b to window parameters wl, ww
def convert_weights_to_window_params(w,b,upbound_value=UPBOUND_VALUE, window_function=WINDOW_FUNCTION):
  if window_function == "relu":
    return convert_weights_to_window_params_relu(w, b,upbound_value=upbound_value)
  if window_function == "sigmoid":
    return convert_weights_to_window_params_sigmoid(w, b, smooth=None, upbound_value=upbound_value)


def convert_weights_to_window_params_sigmoid(w, b, smooth=None, upbound_value=UPBOUND_VALUE):
  if smooth is None:
    smooth = upbound_value / 255.0

  wl = -1.*b/w
  ww = 2./w * np.log(upbound_value/smooth - 1.)
  print("corresponding windowing setting for the windowed image: WL={}, WW={}".format(wl, ww))
  return wl, ww


def convert_weights_to_window_params_relu(w,b, upbound_value=UPBOUND_VALUE):
  wl = upbound_value/(2.*w) - b/w
  ww = upbound_value / w
  print("corresponding windowing setting for the windowed image: WL={}, WW={}".format(wl, ww))
  return wl, ww

"""**Get value distribution of slice**"""

def get_distribution(x):
  #plt.hist(x[0,:,:,0].flatten())
  #plt.show()
  pixel_values = list(filter(lambda e: e!=0, x[0,:,:,0].flatten()))
  plt.hist(pixel_values)
  plt.show

"""# **TRAIN MODEL**

Get data
"""

tm_slices, Y, max_hus = load_data()

def split(tm_slices, rescale=RESCALE):
  if rescale == "global max":
    ### rescale with global max value###
    X_train, X_test, Y_train, Y_test = sklearn.model_selection.train_test_split(rescale_data(tm_slices), Y, test_size=0.1, random_state=RANDOM_STATE)
  elif rescale == "none":
    ### no rescale ###
    X_train, X_test, Y_train, Y_test = sklearn.model_selection.train_test_split(tm_slices, Y, test_size=0.1, random_state=RANDOM_STATE)
  elif rescale == "patient max":  
    ### rescale according to patients###
    X_train, X_test, Y_train, Y_test = sklearn.model_selection.train_test_split(rescale_data_for_pat(tm_slices, max_hus), Y, test_size=0.1, random_state=RANDOM_STATE)
  elif rescale == "by slice":
    X_train, X_test, Y_train, Y_test = sklearn.model_selection.train_test_split(rescale_data_by_slice(tm_slices), Y, test_size=0.1, random_state=RANDOM_STATE)

  return X_train, X_test, Y_train, Y_test


X_train, X_test, Y_train, Y_test = split(tm_slices)

# check the weights before training
init_w, init_b = get_weights(model)

"""Fit model"""

# fit model
es = keras.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', verbose=1, patience=EARLY_STOP, restore_best_weights=True)
history = model.fit(X_train,Y_train, validation_split=0.2, batch_size=BATCH_SIZE, epochs=EPOCHS, shuffle=True, callbacks=[es])

"""visualize accuracy and loss"""

acc = history.history['accuracy']
val_acc = history.history['val_accuracy']

loss = history.history['loss']
val_loss = history.history['val_loss']

plt.figure(figsize=(8, 8))
plt.subplot(2, 1, 1)
plt.plot(acc, label='Training Accuracy')
plt.plot(val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.ylabel('Accuracy')
plt.title('Training and Validation Accuracy')

plt.subplot(2, 1, 2)
plt.plot(loss, label='Training Loss')
plt.plot(val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.ylabel('Loss')
plt.title('Training and Validation Loss')
plt.xlabel('epoch')
plt.show()

"""evaluate model"""

loss, accuracy = model.evaluate(X_test, Y_test, verbose=2)
print("Test loss = {}, Test accuracy = {}".format(loss, accuracy))

"""**Output**: get window parameters wl, ww"""

w, b = get_weights(model)
wl,ww = convert_weights_to_window_params(w,b)